{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9140414",
   "metadata": {},
   "source": [
    "# OpenAI and Ollama integration ü§ñ\n",
    "\n",
    "first of all we need to install the needed/required libraries in order to make our solution works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f4409",
   "metadata": {},
   "source": [
    "## 1 - install and import\n",
    "\n",
    "first let's install needed libraries\n",
    "\n",
    "**ollama** : used to communicate with our local ollama server\n",
    "\n",
    "**openai** : used to communicate with openai model in the cloud\n",
    "\n",
    "***N.B:*** we can use openai also to communicate with ollama or another models (like claude) but for training purpose we are going to use both of them\n",
    "\n",
    "**gradio**: ready to use components developed by huggingface that enable to used modern ui component for ML usage\n",
    "\n",
    "**python_dotenv**: help load envs variables from the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a37d76",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /opt/anaconda3/lib/python3.12/site-packages (0.4.7)\n",
      "Requirement already satisfied: python_dotenv in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.73.0)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.32.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /opt/anaconda3/lib/python3.12/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ollama) (2.11.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.13.2)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.10.2 (from gradio)\n",
      "  Downloading gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.18-cp312-cp312-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (10.4.0)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.12-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Using cached tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from gradio-client==1.10.2->gradio) (2024.6.1)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.10.2->gradio)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.28.1->gradio)\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Downloading gradio-5.32.1-py3-none-any.whl (54.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
      "Downloading orjson-3.10.18-cp312-cp312-macosx_15_0_arm64.whl (133 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.12-py3-none-macosx_11_0_arm64.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Installing collected packages: pydub, websockets, uvicorn, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, hf-xet, groovy, ffmpy, aiofiles, starlette, huggingface-hub, typer, safehttpx, gradio-client, fastapi, gradio\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.11.1\n",
      "    Uninstalling tomlkit-0.11.1:\n",
      "      Successfully uninstalled tomlkit-0.11.1\n",
      "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.6.0 gradio-5.32.1 gradio-client-1.10.2 groovy-0.1.2 hf-xet-1.1.2 huggingface-hub-0.32.3 orjson-3.10.18 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.12 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.46.2 tomlkit-0.13.2 typer-0.16.0 uvicorn-0.34.3 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ollama python_dotenv openai gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085009f4",
   "metadata": {},
   "source": [
    "## 2 - import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f14408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5a95d",
   "metadata": {},
   "source": [
    "after calling the imports we need know to load the envs and start a sample call to open ai first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590709e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file and override existing ones\n",
    "load_dotenv(override=True)\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc44591",
   "metadata": {},
   "source": [
    "before diving into using our first call with our model we need to know how llm model prompts works:\n",
    "a prompt can be divided into 3 role: system, user, assistant.\n",
    "\n",
    "**1 - System**:\n",
    "\n",
    "a system prompt is like a guide to our llm generation that tell it how to behave in each generation for example `you are a helpful doctor assistant, please analyze my symptoms and suggest the best health advice.` here the llm will always behave as a doctor and will see if the generation match the goal of the system prompt and be aligned with it\n",
    "\n",
    "**2 - User**: \n",
    "\n",
    "a user prompt is basically your regular prompt that you entre during you chat with llm in order to have a result for example `i feel tired and sleepy and also i don't have appetite to eat`\n",
    "\n",
    "**3 - Assistant**:\n",
    "\n",
    "same as user role but with extra assistance feature like keep providing question in order to assist you or for more clarification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da946756",
   "metadata": {},
   "source": [
    "## 3 - let's call the llmüìù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856e9ef",
   "metadata": {},
   "source": [
    "first let's make a function that will be used later with gradio üòÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc909e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocChat(prompt, history):\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"you are a helpful doctor assistant, please analyze my symptoms and suggest the best health advice. without any hillucination and being out of context if you don't know the answer just say you don't know.\"\n",
    "    }] + history\n",
    "    final_prompt = f\"\"\"\n",
    "    this is an medical chat, please analyze the symptoms and suggest the best health advice from user input's\n",
    "    ---\n",
    "    {prompt}\n",
    "    ---\n",
    "    thanks for you help.\n",
    "    \"\"\"\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": final_prompt\n",
    "    })\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c40727",
   "metadata": {},
   "source": [
    "after making this function called `DocChat` let's use gradio chat interface component for testing our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d7e6bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(DocChat, type=\"messages\", title=\"Medical Chat Assistant\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73cb199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dece90b2",
   "metadata": {},
   "source": [
    "## 4 - Ollamaü¶ô this time üßê?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "782f91ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://c00c8753a3144ec12c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c00c8753a3144ec12c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def DocChatOllama(propmt, history):\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"you are a helpful doctor assistant, please analyze my symptoms and suggest the best health advice. without any hillucination and being out of context if you don't know the answer just say you don't know.\"\n",
    "    }] + history\n",
    "    final_prompt = f\"\"\"\n",
    "    this is an medical chat, please analyze the symptoms and suggest the best health advice from user input's\n",
    "    ---\n",
    "    {propmt}\n",
    "    ---\n",
    "    thanks for you help.\n",
    "    \"\"\"\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": final_prompt\n",
    "    })\n",
    "    stream = chat(\n",
    "        model=\"gemma3:1b\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk['message']['content']:\n",
    "            response += chunk['message']['content']\n",
    "            yield response\n",
    "gr.ChatInterface(DocChatOllama, type=\"messages\", title=\"Medical Chat Assistant (Ollama)\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480ab1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
